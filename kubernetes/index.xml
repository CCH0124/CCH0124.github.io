<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Kubernetes on</title><link>https://cch0124.github.io/kubernetes/</link><description>Recent content in Kubernetes on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sat, 23 Oct 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://cch0124.github.io/kubernetes/index.xml" rel="self" type="application/rss+xml"/><item><title>kubernetes 之訪問安全控制 - RBAC</title><link>https://cch0124.github.io/kubernetes/2021-10-23-rbac/</link><pubDate>Sat, 23 Oct 2021 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/kubernetes/2021-10-23-rbac/</guid><description>RBAC(Role-Based Access Control)，它將權限授予角色(role)上，可以想像它是一個責任。對 RBAC 來說，使用者(User)是一個獨立可存取資源的主體(Subject)。這過程中被允許對一或多個 Object 執行的操作可以稱做許可(Permission)，一個使用者可藉由授權而擁有多個 role。
人 O \|/ / \ action(verb) Subject --------------------&amp;gt; Object RBAC 中 User、Role 和 Permission 關係如下
___________________________ -----&amp;gt; Role | Permissions | / \ | | User -----&amp;gt; | Operations -----&amp;gt; Objects | \ / | | -----&amp;gt; Role |___________________________| RBAC 是一個限定操作的機制，用於定義誰(subject)能或不能操作(verb)哪個物件(object)。動作的發出者(subject)可以是一個 User Accoun 或是 Service Account；verb 表示要執行的操作 create、apply、delete、update、patch、edit 和 get 等；object 是指要被操作的目標資源，以 Kubernetes API 來看是以 URL 作為對象。
RBAC 支援 Role 和 ClusterRole 兩種角色，前者是 namespace 級別後者則是集群級別，對這兩類給權限時，需要用到 RoleBinding 和 ClusterRoleBinding。RoleBinding 將 role 上的 Permissions 綁定到一個或一組使用者上，此綁定只能隸屬於某一個 namespace。ClusterRoleBinding 則用於及群集別。</description></item><item><title>kubernetes 之訪問安全控制</title><link>https://cch0124.github.io/kubernetes/2021-10-16-security/</link><pubDate>Sat, 16 Oct 2021 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/kubernetes/2021-10-16-security/</guid><description>Controlling Access to the Kubernetes API API Server 是存取和管理資源對象的入口，不管是 kube-controllermanager、kube-scheduler、kubelet 和kube-proxy 等都要透過 API Server 進行存取。而每一次的訪問請求都須進行合法的驗證，如身分、操作權限等，當這些流程都為正常才能將書據存入 etcd 中。當請求到 API 時，會經歷幾個階段，如下圖所示：
當收到一個用戶端的請求後，會調用 Authentication 來驗證用戶端身分，如果前者驗證通過接著會驗證 Authorization 是否有權限去操作用戶端發送的請求（建立、讀取、刪除等），如果授權(Authorization)通過驗證必須在通過 Admission Control 檢測像是 namespace 是否存在、使否違反資源限制等。
用戶端存取 API 可以透過 kubectl、函式庫或使用 REST 方式，然而可以操作的主體被分為人和 POD 物件，其分別對應 User Account 和 Service Account。
User Account 非 kubernetes 所使用的管理帳號，像是密鑰、Keystone 或是以檔案方式的使用者和密碼列表 名稱需是唯一值 Service Account 是 Kubernetes API 所管理的帳號，使用在 POD 之中的服務行程訪問 Kubernetes API 時提供的身分標識 一般會綁定特定 namespace，會附帶 Secret 資源的憑證用於訪問 API Server 上面兩種類型都可隸屬一或多個用戶組，而用戶組本身沒有操作權限，其本身只是一個 User Account 的邏輯集合。Kubernetes 有以下特殊目的的組</description></item><item><title>kubernetes - day27</title><link>https://cch0124.github.io/kubernetes/2020-09-17-etcd/</link><pubDate>Thu, 17 Sep 2020 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/kubernetes/2020-09-17-etcd/</guid><description>etcd 架構以及內部機制 一個 etcd 的集群節點之間透過 Raft 一致性演算法完成分散式一致性協同，從圖中可以知道會有一個 leader，而當該 leader 故障時，會自動再選取其它節點做為 leader，同時也完成數據的同步，對於客戶端來說只要選取任一節點即可做讀寫操作，內部的狀態和數據協同由 etcd 自身完成。etcd 中還有一個 quorum 概念，其表示容忍故障的數量。
客戶端對 etcd 做操作時只需簡單的使用 HTTP 方式即可存取。對於其數據可以想成是鍵值做一個儲存。同時 etcd 為了使用戶端訂閱數據變更，支援 watch 機制，透過 watch 即時獲取 etcd 中數據的增量更新，從而實現與 etcd 中的數據同步等業務邏輯。
etcd 主要提供了一下接口
Put(key, value), Del(key, value) Get(key), Get(keyFrom, keyEnd) Watch(key/keyPrefix) Transactions(if/then/else ops.).Commit() Leases: Grant/Revoke/KeepAlive etcd 版本機制 etcd 中有 term 的概念，表示整個集群 Leader 的任期。只要 Leader 發生變化就會加 1。再者就是 revision，表示全域數據版本，當數據發生變更，包括創建、修改、刪除等，其 revision 都會加 1。不過 Leader 發生切換時 revision 是延續的。
使用場景 Server Discovery （Naming Service） Distributed Coordination: leader election Distributed Coordination Kubernetes 中的 etcd 在 Kubernetes 中元件間的通訊都是藉由 API Server 通訊，而 API Server 是和 etcd 通訊的唯一元件，因此在 Kubernetes 上所有狀態都是藉由 API Server 來修改。從下面這個系統上預設元件來看，etcd、coredns、apiserver、controller-manager 和 scheduler 都運行在 master 上，我們所下達的 kubectl 相關命令都是 API Server 向 kubelete(每個節點都會安裝的代理) 發起。</description></item><item><title>kubernetes - day26</title><link>https://cch0124.github.io/kubernetes/2020-09-16-statefulset/</link><pubDate>Wed, 16 Sep 2020 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/kubernetes/2020-09-16-statefulset/</guid><description>StatefulSet 簡單來說就是管理有狀態的應用程式，它使得每個應用程式都有不可替換的資源個體，其每個 POD 都有固定的 hostname 和專屬 volume，即時重新調度也不影響。
Stateful 和 Stateless 一個應用程式是否要紀錄前一次或多次連線中的內容訊息做為下一次連線的資訊，而該連線的對象可能是設備、用戶端或其它應用程式等。透過是否紀錄來分辨有狀態與無狀態，前者為需要記錄動作，後者則不用。
StatefulSet 與 ReplicaSet StatefulSet 在文章開頭大致上將它的重點說明了。但我們可以比較它和 ReplicaSet。ReplicaSet 隨著被資源調度就會被新的資源所取代，因為其網路等資源被改變，StatefulSet 則是就算被重新調度，其源個體都會有著相同的資源。同樣的 StatefulSet 有 ReplicaSet 的 replicas 功能，但和 ReplicaSet 生成的 POD 副本卻是不一樣的，StatefulSet 中的 POD 會有獨立的 PV 也就是儲存空間，另一個是 POD 名稱，它會使用編號方式去命名，而 StatefulSet 也支持滾動更新，基於這些功能 StatefulSet 都會講求順序，在後面範例就會明白。
StatefulSet 特性 網路標識 通常一個 StatefulSet 會在建立一個 headless Service 資源，用來記錄每個 POD 的網路標識，就像先前文章講的，每一個 POD 都會有一個獨立的 DNS 紀錄，使得客戶端能夠透過 hostname 去找到服務。
資源調度 當 StatefulSet 下的 POD 發生故障時，會像 ReplicaSet 一樣將其重新建立，但是不一樣的是 StatefulSet 會讓該新 POD 擁有之前 POD 的 hostname 等，因此透過該 hostname 進行訪問時還是會存取到一樣的 POD 資源。</description></item><item><title>kubernetes - day25</title><link>https://cch0124.github.io/kubernetes/2020-09-15-configmap-secret/</link><pubDate>Tue, 15 Sep 2020 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/kubernetes/2020-09-15-configmap-secret/</guid><description>這邊在大致說明 ConfigMap 和 Secret，它們也屬於 volume 的一種，前者通常是以配置的數據等為主；後者則是以密鑰等為主。為什麼說是 volume 的一種呢？透過掛載方式那些像是 TLS、SSL、CA 證書或一些配置可以與容器的 image 做到解偶。在傳統的 Docker 來說，一個容器中的應用程式不大可能使用預設配置，因此我們會透過環境變數或卷(volume) 等方式進行配置。在 K8s 上也利用 ConfigMap 和 Secret 完成一些容器所需的配置，透過這種方式也可避免被重新調度後導致配置內容遺失的問題。
容器參數配置 透過 explain 方式，可以在容器等級字段中找到 args、command 的屬性，它們是用來定義容器要運行的指令(command)和傳遞參數(args)。官方也有說明，command 對應於 Dockerfile 中的 ENTRYPOINT；args 則對應於 CMD。
The command field corresponds to entrypoint in some container runtimes. Refer to the Notes below. The environment variable appears in parentheses, &amp;ldquo;$(VAR)&amp;rdquo;. This is required for the variable to be expanded in the command or args field.</description></item><item><title>kubernetes - day24</title><link>https://cch0124.github.io/kubernetes/2020-09-14-volume-storage-class/</link><pubDate>Mon, 14 Sep 2020 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/kubernetes/2020-09-14-volume-storage-class/</guid><description>在 K8s 中 StorageClass 對象的目的，就是不去創建 PV 而是透過 PVC 需求去建立 PV，這表示不必要再去管理 PV 資源，相較於 PVC 請求 PV 方式帶來更高的靈活性。
在創建 StorageClass 對象時，name 的定義一樣是重要的，PVC 在調用時會使用 storageClassName 去對應該 StorageClass 對象的 name，創建時還需定義以下通用字段
provisioner 提供儲存的儲存系統，也就是供應商 parameter 會依照 provisioner 的不同而有不同的參數 reclaimPolicy PV 回收策略，預設是 Delete 否則可定義 Retain volumeBindingMode 定義讓 PVC 完成提供和綁定資源 GKE 上 StorageClass 實作 我們以 Nginx 為例，為它建立一個 StorageClass。我們會定義以下的 yaml。volumeBindingMode 字段可參考官方。
apiVersion: storage.k8s.io/v1 kind: StorageClass # 定義 StorageClass metadata: name: standard-us-centrall-a-b-c provisioner: kubernetes.</description></item><item><title>kubernetes - day23</title><link>https://cch0124.github.io/kubernetes/2020-09-13-persistent-volume/</link><pubDate>Sun, 13 Sep 2020 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/kubernetes/2020-09-13-persistent-volume/</guid><description>PersistentVolume(PV)是由管理者提供並配置在某一個儲存方案的一個空間，它將儲存抽象成一個可讓使用者去申請的資源。以前面的 NFS 為例，我們都在 POD 中直接定義關於儲存的細節像是 IP 等，在這的做法缺少靈活性，當儲存要變換 IP 或是儲存的路徑，會相對的麻煩。因此可藉由 PV 變成是 POD 和儲存方案中間的抽象層，這使得 POD 不需要知道儲存的細節，這全部由 PV 去定義連接與使用，而 PV 使用需要透過 PersistentVolumeClaim(PVC) 描述的資源來完成綁定，也就是向 PV 申請儲存空間大小或是存取權限。其整體示意圖如下
講完了 PV 和 PVC 概念後這邊來實作，環境是基於上一章。
建立 PV 資源 PersistentVolume 的定義有以下常見字段
Capacity PV 儲存空間大小定義 AccessMode 存取模式有以下值，詳細看官方當中會說明有儲存方案支援哪些模式 ReadWriteOnce ReadOnlyMany ReadWriteMany persistentVolumeReclaimPolicy PVC 移除時對應的 PV 動作 Retain 保存資料 預設值 Delete 對應的 PV 資源和資料一同刪除 Recycle 保留 PV，移除資料 volumeMode 指定為 Filesystem 或是其它，預設為 Filesystem storageClassName PV 所屬的 StorageClass 的名稱，默認為空值表示不屬於任何 其字段還有很多可搭配，因為太多東西因此學起來會需要時間&amp;hellip;。我們開始實驗吧!</description></item><item><title>kubernetes - day22</title><link>https://cch0124.github.io/kubernetes/2020-09-12-volume-nfs/</link><pubDate>Sat, 12 Sep 2020 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/kubernetes/2020-09-12-volume-nfs/</guid><description>接續上一章的實作，這邊將使用 NFS(Network File System) 分散式儲存系統實現永久儲存，它可以讓客戶端像是在本地端取得資料。
NFS 儲存卷 這邊將不使用 GKE 操作而是使用本地端架設的 K8s 來操作，將會有四台虛擬機，一台 NFS，其它則為 K8s 叢集。我們會在一台虛擬機上安裝 NFS，並設定要導出的儲存空間，之後再藉由設定檔方式去讓 POD 取得 NFS 的掛載目錄。在 POD 生命週期中，此方式只是會卸載掛載的資訊，並非像 emptyDir 一樣是直接刪除。其定義字段如下
server: NFS Server 的 IP 或是能解析的域名 path: NFS Server 定義的共享檔案路徑 readOnly: 是否要唯讀，預設是 false 先在 NFS 的虛擬機上安裝 NFS 環境，在 NFS Server 主機安裝 nfs-kernel-server，客戶端安裝 nfs-common也就是 K8s 叢集的節點，安裝完之後在 NFS Server 設定以下
sudo apt install -y sudo mkdir /k8sData echo &amp;#34;/home/cch/k8sData *(rw,sync,no_root_squash)&amp;#34; | sudo tee /etc/exports # , 不能有空格隔開 sudo exportfs -r # reload sudo showmount -e # 顯示 NFS 設定的要掛載目錄 K8s 的叢集節點進行 NFS 的掛載</description></item><item><title>kubernetes - day21</title><link>https://cch0124.github.io/kubernetes/2020-09-11-volume/</link><pubDate>Fri, 11 Sep 2020 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/kubernetes/2020-09-11-volume/</guid><description>Kubernetes 提供的儲存方案屬於 POD 的資源，一個 POD 中的多個容器可以一同共享其儲存數據。而在容器中的數據可能會隨著 POD 生命週期而消失，在 Kubernetes 上的儲存方案可以實現出在 POD 生命週期外的儲存。接下來將會介紹 Kubernetes 上的儲存應用。
volume 概念 就像開頭講的 POD 的生命週期無法讓容器可以永久儲存數據。以 Docker 來說它可以支持網路檔案系統或是一般的檔案系統以掛載方式作永久性儲存，而在 Kubernetes 中也為 POD 提供相似的功能，這功能使得容器可以可掛載 POD 設定的外部儲存設備方案，也可達到跨節點的需求，至於是否要持久化儲存，取決於設定。示意圖如下。
Kubernetes 的儲存方案 Kubernete 所支援的儲存方案非常的多，儲存方面還支援了 ConfigMap、Secret 這些對於隱密資訊或一些變數的儲存。從官方可以看有很多儲存類型，不論是分散式或是雲端儲存。
官方中的 emptyDir 類型隨著 POD 生命週期變化；hostPath 則是以主機的目錄關聯至 POD，只要被重新調度，就無法使用，以持久化來看這兩個類型並非是持久的。相對的持久化的類型都要是網路儲存系統像是 NFS、Ceph 甚至是雲端的儲存方案。Kubernetes 中儲存有 PersistentVolume(PV) 和 persistentVolumeClaim(PVC) 的概念，PV 可借助管理者配置其儲存方案；PVC 則是去請求那些 PV，這過程簡化了配置儲存方案的複雜度。
前面有題到兩個特殊類型的 volume 分別是 ConfigMap 和 Secret，以下進行簡略介紹
ConfigMap 為 POD 寫入而外訊息，將數據定義至 ConfigMap 對象中。POD 需要時則在資源清單中引用即可 Secret 儲存較隱密的資訊用，需使用時在將其掛載至 POD 中，這樣避免了在製作 image 時隱密資訊的寫入 下面會先實作 emptyDir 與 hostPath，下一章節會在講 NFS 的實驗。</description></item><item><title>kubernetes - day20</title><link>https://cch0124.github.io/kubernetes/2020-09-10-service-discovery/</link><pubDate>Thu, 10 Sep 2020 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/kubernetes/2020-09-10-service-discovery/</guid><description>前面章節講過的 Service，提供了一個穩定可讓客戶端取得想要的服務接口，這之間 POD 如何知道某特定服務的 IP 和 Port 呢？這就需要服務發現(Service Discovery) 的幫助。我們也知道在 K8s 環境中，POD 不能依靠網路，當服務重新部署時，會導致 IP 不同，因此需要借助服務發現。服務發現簡單來說就是是弄清楚如何連接到服務的實際過程。
環境變數的服務發現 Service 環境變數 只要創建 Service 都會使用以下的環境變數，當在同一 namespace 的 POD 會自動擁有這些資訊。可參考官網 container-environment
{SVCNAME}_SERVICE_HOST {SVCNAME}_SERVICE_PORT $ kubectl exec httpd-7765f5994-97hq5 -- printenv PATH=/usr/local/apache2/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin HOSTNAME=httpd-7765f5994-97hq5 MYAPP_SVC_PORT_80_TCP_ADDR=10.8.2.52 MYAPP_SVC_SERVICE_HOST=10.8.2.52 # this KUBERNETES_PORT_443_TCP_ADDR=10.8.0.1 MYAPP_SVC_PORT_80_TCP_PORT=80 KUBERNETES_PORT_443_TCP_PROTO=tcp KUBERNETES_PORT_443_TCP_PORT=443 MYAPP_SVC_PORT_80_TCP_PROTO=tcp KUBERNETES_SERVICE_PORT=443 KUBERNETES_SERVICE_PORT_HTTPS=443 KUBERNETES_PORT=tcp://10.8.0.1:443 KUBERNETES_PORT_443_TCP=tcp://10.8.0.1:443 MYAPP_SVC_SERVICE_PORT=80 # this MYAPP_SVC_PORT=tcp://10.8.2.52:80 MYAPP_SVC_PORT_80_TCP=tcp://10.8.2.52:80 ... Docker Link 方式環境變數 這邊我在 GCP 上的 Compute Engine 拿一個節點的容器用 docker inspect 觀察。
$ docker inspect 203e907bf5ba &amp;#34;Env&amp;#34;: [ &amp;#34;MYAPP_SVC_PORT=tcp://10.</description></item><item><title>kubernetes - day19</title><link>https://cch0124.github.io/kubernetes/2020-09-09-service-ingress-part3/</link><pubDate>Wed, 09 Sep 2020 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/kubernetes/2020-09-09-service-ingress-part3/</guid><description>Loadbalance 類型的 Service 上一章節實作了 nodePort 類型的資源，它可讓集群外部存取，但 nodePort 資源有著一個缺點，要存取時須知道集群中至少一個節點 IP，該結點如果故障得要取得其它節點的 IP。這問題可藉由在集群外部部署一個負載均衡器，可以經由它存取外部客户端請求同時調度到集群中相應的 nodePort，這類型的資源是 LoadBalancer。我們透過 GKE 環境實作。
apiVersion: v1 kind: Service metadata: name: loadbalance-service spec: type: LoadBalancer selector: app: hello-kubernetes ports: - protocol: TCP port: 8080 targetPort: 9376 nodePort: 32223 apiVersion: apps/v1 kind: Deployment metadata: name: hello-kubernetes-loadbalance spec: replicas: 3 selector: matchLabels: app: hello-kubernetes template: metadata: labels: app: hello-kubernetes spec: containers: - name: hello-kubernetes image: paulbouwer/hello-kubernetes:1.7 ports: - containerPort: 8080 --- apiVersion: v1 kind: Pod metadata: name: pod-client labels: app: client spec: containers: - name: client image: hwchiu/netutils 查看建立的 LoadBalancer 資源，此資源會分配一個對外存取的 IP(EXTERNAL-IP)。</description></item><item><title>kubernetes - day18</title><link>https://cch0124.github.io/kubernetes/2020-09-08-service-ingress-part2/</link><pubDate>Tue, 08 Sep 2020 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/kubernetes/2020-09-08-service-ingress-part2/</guid><description>預設的 Service 這預設是讓節點能夠跟 API Server 等資源做通訊。
$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.8.0.1 &amp;lt;none&amp;gt; 443/TCP 15d $ kubectl describe svc kubernetes Name: kubernetes Namespace: default Labels: component=apiserver provider=kubernetes Annotations: &amp;lt;none&amp;gt; Selector: &amp;lt;none&amp;gt; Type: ClusterIP IP: 10.8.0.1 Port: https 443/TCP TargetPort: 443/TCP Endpoints: 34.66.217.13:443 Session Affinity: None Events: &amp;lt;none&amp;gt; clusterIP 類型的 Service 同樣的使用 nginx 做為範例。下面是實驗的 yaml。在前面文章有提到過，使用 expose 也可以達到創建 Service 資源。
apiVersion: apps/v1 kind: Deployment metadata: name: my-nginx spec: replicas: 5 selector: matchLabels: run: my-nginx template: metadata: labels: run: my-nginx role: backend spec: containers: - name: my-nginx image: nginx:1.</description></item><item><title>kubernetes - day17</title><link>https://cch0124.github.io/kubernetes/2020-09-07-service-ingress/</link><pubDate>Mon, 07 Sep 2020 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/kubernetes/2020-09-07-service-ingress/</guid><description>POD 並非有持久性，可能出於各種原因重新調度 POD，像是失敗的 liveness 或 readiness 檢查，如果此時與 POD 通訊會怎樣？當 POD 重啟時，可能具有不同的 IP 地址。這就是為什麼有 Service 的資源，Service 用於為 POD 提供一個固定、統一的存取功能和負載均衡，同時在集群內部使用 DNS 實現服務發現，解決客戶端發現容器的問題。Service 和 POD 的 IP 只在 Kubernetes 集群内相互存取，無法直接干預外部流量的請求。而 Kubernetes 提供了一些方法像是 hostPort、hostNetwork、NodePort 或 LoadBalancer 等，而另一種 Ingress 則是資源第七層的均衡負載。Service 藉由規則去定義策略，最重要的是還是會藉由標籤去實現。
Service 資源 上述描述的問題，是編排系統可能會遇到的問題。當 POD 在做伸縮的應用時或是重新調度，都有可能影響 IP，這會導致客戶端存取資源時會錯誤。為了解決此問題 kubernetes 提供了 Service 資源。前面也提到說 Service 還是會藉由標籤去實作，如下圖所示，同時 Service 也隱藏了真實處裡用戶請求的 POD。下圖的標籤選擇器有多個符合的後端，這會讓 Service 可以用負載均衡方式進行調度的處裡。
Service 會提供給 POD 的存取等級，這取決於服務的類型，當前有三種類型：
ClusterIP 屬於集群內部，只有集群內部的資源可相互存取，默認選項 NodePort 為節點提供一個可訪問的 IP 與 Port LoadBalancer 從雲提供商添加負載均衡器，將流量從服務轉發到服務中的節點 詳細可參考官網。實際上 Service 並非直接與 POD 通訊，其之間還有叫 Endpoints 的資源，它是由 IP 地址和 Port 组成的，Service 會擁有這個資源是透過標籤選擇器匹配的 POD 取得，這部分 K8s 幫我們自動實現了。</description></item><item><title>kubernetes - day16</title><link>https://cch0124.github.io/kubernetes/2020-09-06-job-cronjob/</link><pubDate>Sun, 06 Sep 2020 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/kubernetes/2020-09-06-job-cronjob/</guid><description>Job 控制器是一次性任務的 POD 控制器，期會創建一至多個 POD，只要容器運行過程正常的運行即不會有重啟的動作，否則需要依照重啟策略去運行，如果遇到節點故障未完成任務的 POD 則會被重新的調度。Job 完成任務的定義是 Job 控制器會記錄 POD 成功執行完任務的個數，並達到成功次數的值。運行 Job 方式可以是是否要以平行的方式去運行多個 POD。CronJob 可以想成是 crontab，用來管理 Job 控制器要運行的時間，也就是在未來的某一個時間上運行或是固定某一時段運行。前面文章所提到的控制器會比較希望應用程式是以一個 Daemon 來運行，而 Job 類型則是會以 Task 為主的應用程式。
非平行化 一次性 一次執行一個 POD 作業，直到達到定義成功的次數 平行化 平行處理 會設置工作列隊的數量，該數量可讓多個 POD 同時作業 建立 Job Job 需要定義必要的 template，可透過 kubectl explain job.spec 去查看，至於標籤選擇器則會自動藉由 template 去創建。以下我們拿官網的範例實驗
apiVersion: batch/v1 kind: Job metadata: name: pi spec: template: spec: containers: - name: pi image: perl command: [&amp;#34;perl&amp;#34;, &amp;#34;-Mbignum=bpi&amp;#34;, &amp;#34;-wle&amp;#34;, &amp;#34;print bpi(2000)&amp;#34;] restartPolicy: Never # backoffLimit: 4 Job 建立 POD 無法使用 restartPolicy 的默認值(always)，因為 Job 並非要無限期運作。需要設置 Never 或 OnFailure 防止容器完成任務後重新啟動</description></item><item><title>kubernetes - day15</title><link>https://cch0124.github.io/kubernetes/2020-09-05-daemonset/</link><pubDate>Sat, 05 Sep 2020 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/kubernetes/2020-09-05-daemonset/</guid><description>DaemonSet 簡稱 ds 也是一個 POD 控制器，用於實現在集群中每個節點運行一份 POD，即使是後續加入的節點，而移除節點則會進行 POD 的回收。假設只需針對某一些節點上進行部署，則可以使用節點的選擇器或是以標籤方式做限制。DaemonSet 使用場景可能有以下
Log 搜集器 集群類型的儲存 資源監控相關 下圖為 Kubernetes in action 一書，比較 ReplicaSet 和 DaemonSet 差異 實驗環境是使用 GKE 我們可以查看預設下有哪些原件是使用 DaemonSet
$ kubectl get ds -n kube-system -l name!=filebeat NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE fluentd-gcp-v3.1.1 4 4 4 4 4 beta.kubernetes.io/fluentd-ds-ready=true,beta.kubernetes.io/os=linux 14d metadata-proxy-v0.1 0 0 0 0 0 beta.kubernetes.io/metadata-proxy-ready=true,beta.kubernetes.io/os=linux 14d nvidia-gpu-device-plugin 0 0 0 0 0 &amp;lt;none&amp;gt; 14d prometheus-to-sd 4 4 4 4 4 beta.</description></item><item><title>kubernetes - day14</title><link>https://cch0124.github.io/kubernetes/2020-09-04-deployment/</link><pubDate>Fri, 04 Sep 2020 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/kubernetes/2020-09-04-deployment/</guid><description>Deployment 縮寫為 deploy，它被建構在 ReplicaSet 控制器之上的控制器。為 POD 和 ReplicaSet 提供聲明式更新，而 Deployment 基本上和 ReplicaSet 很相似，只不過多了這些特性
可以觀察 Deployment 升級的過程 可以用回滾方式回到歷史版本中的某一個版本 對 Deployment 操作紀錄作保存，以便可回滾 在每一次升級過程中都可暫停或啟動 自動更新機制有兩種 Recreate 一次性刪除所有 POD，之後再用新版本佈署 RollingUpdate 滾動式更新，小階段的更新 Deployment 的建立 其定義的字段與 Replica 很像，如 replicas、selector、template 等。下面是本章節的實驗範例。
# deploy-demo.yaml apiVersion: apps/v1 kind: Deployment metadata: name: myapp-deploy namespace: default spec: replicas: 5 selector: matchLabels: app: myapp release: canary template: metadata: labels: app: myapp release: canary spec: containers: - name: myapp image: nginx:1.</description></item><item><title>kubernetes - day13</title><link>https://cch0124.github.io/kubernetes/2020-09-03-replicaset/</link><pubDate>Thu, 03 Sep 2020 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/kubernetes/2020-09-03-replicaset/</guid><description>ReplicaSet 縮寫為 RS，是一種 POD 控制器。用來確保所管控的 POD 副本數在任一時間都能保證用戶所期望的需求。下圖演示了 RS 操作，RS 觸發後會去找匹配 selector 的 POD，當前啟動的數量與期望的數量不符合時，會進行一些操作，像是多的數量則刪除，少的話透過 POD 模板建立。當期數量符合用戶定義時則不斷的循環。
from Kubernetes in action
RS 的建立通常以三個字段定義，selector、replicas 和 template，如下圖所示。當中 selector、replicas 或 template 隨時可按照需求做更改。replicas 的定義數量為其直接影響；selector 可能讓當前的標籤不再匹配，讓 RS 不再對當前的 POD 進行控制；template 的修改是對後續創建新 POD 才產生影響。
from Kubernetes in action
對於一般手動建立 POD，RS 帶來的好處有以下
確保 POD 數量符合定義數量 當節點發生故障時，自動請求自其它節點創建缺失的 POD POD 的水平縮放 必要時可透 HPA(HroizontalPodAutoscaler) 控制器針對資源來自動縮放
建立 RS 再前面描述有說明 RS 重要的資源創建字段有哪些。這邊再說明一個字段 minReadySecond 預設值為 0 秒，在建立新 POD 時，啟動後有多長時間無出現異常就可被視為READY 狀態。以下為建立一個 RS 的範例。</description></item><item><title>kubernetes - day12</title><link>https://cch0124.github.io/kubernetes/2020-09-02-pod-controller/</link><pubDate>Wed, 02 Sep 2020 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/kubernetes/2020-09-02-pod-controller/</guid><description>POD 的調度過程，前面文章都有提到過。kubelete 對於容器的非主行程錯誤無法去感覺，需要依賴於 liveness probe 機制。再想想，如果 POD 被惡意刪除或是節點出現問題又該如何解決 ? 我們都知道 kubelet 是運行在每個節點上的必要代理器，因此節點故障，對於 POD 的各種資源都無法有保證性。對於這些問題我們必須使用節點外的 POD 控制器實現該保證性。
POD 控制器由主節點上的 kube-controller-manager 實現，常見的控制器有 ReplicaSet、Deployment、DaemonSet、StatefulSet、Job 等，它們實現不同的想法來管理 POD 資源。當然 API Server 不能少，它將負責儲存使用者的清單資源，再由控制器去實現使用者想要的狀態，在這之間控制器會透過 API Server 提供的接口進行不斷的監聽資源狀態，因此發生故障、更新等變動系統狀態的原因會不斷的向用戶想要的狀態不斷接近，而 status 的狀態就是紀錄當前狀態。
控制器與 POD 正常來說，一個 POD 控制器資源應該至少有三個字段
selector 關聯匹配的 POD，並管控其 POD replica 期望的 POD 數量 POD Template 指定控制器創建 POD 資源的配置訊息 相似於定義 POD 資源 結論 此篇文章用於了解，POD 資源被控制器控管的好處，以及如何定義控制器資源。再下面的章節將會介紹一些控制器。
參考資源 feisky - controller-manager</description></item><item><title>kubernetes - day11</title><link>https://cch0124.github.io/kubernetes/2020-09-01-pod-resource-limit-request/</link><pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/kubernetes/2020-09-01-pod-resource-limit-request/</guid><description>在 K8s 上，可經由容器或 POD 請求或使用的計算資源有記憶體和 CPU。相互比較的話前者為不可壓縮資源，做一些伸縮操作可能會有問題，而後者事可壓縮資源，可以做伸縮的操作。
資源隔離目前是屬於容器級別，而資源藉由 requests 定義請求的可最小可用值，另一個 limits 用於限制資源最大可用值，如下圖所示。在 K8s 上，一個單位的 CPU 相當於虛擬機上的一顆 vCPU 或實體機上的一個 Hyperthread(一個邏輯 CPU)，一個核心相當於 1000 個微核心所以200m 相當於 0.2 個核心。記憶體以 Ei、Pi、Ti、Ki 等單位作為計算。
from &amp;ldquo;https://jaxenter.com/manage-container-resource-kubernetes-141977.html&amp;quot;
Example requests apiVersion: v1 kind: Pod metadata: name: stress-pod labels: app: test spec: containers: - name: stress image: ikubernetes/stress-ng command: [&amp;#34;/usr/bin/stress-ng&amp;#34;, &amp;#34;-c 1&amp;#34;, &amp;#34;-m 1&amp;#34;, &amp;#34;--metrics-brief&amp;#34;] resources: requests: memory: &amp;#34;128Mi&amp;#34; cpu: &amp;#34;200m&amp;#34; 配置清單上，POD 要求為容器要有 128Mi 記憶體和 5 分之 1 的 CPU 核心的最小資源。使用 -m 1 進行記憶體的壓測，滿載時盡可能占用 CPU 資源，同時間 -c 1 是對 CPU 進行壓測。</description></item><item><title>kubernetes - day10</title><link>https://cch0124.github.io/kubernetes/2020-08-31-pod-lifecycle/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/kubernetes/2020-08-31-pod-lifecycle/</guid><description>此篇分享是要說關於 POD 對象的生命週期，POD 對象從建立到終止退出為它們的一個生命週期。而在這之間可以透過 POD 的定義執行一些創建容器、初始化容器等操作。在 openshift 中提供該生命週期流程圖，如下所示
這邊使用 openshift 文章內容進行翻譯解釋其流程
啟動其它容器之前，將啟動 infra 容器，以建立其它容器加入的名稱空間，依照此理解應該是指 pause 容器 用戶定義的第一個容器啟動是 init 容器，可將其用於 POD 範圍內的初始化 main 容器和 post-start hook 同時啟動，在範例中為4秒鐘 在秒數為 7 時，再次按每個容器啟動 liveness 和 readiness probes 在秒數為 11 時，當 POD 被終止時，執行 pre-stop hook，最後在寬限期後終止 main 容器 POD 定相 不管 POD 是被手動建立、或是透過一些 POD 控制器建立，POD 應當處於以下幾個定相
Pending
API Server 創建了 POD 資源並存入 etcd 中，但它尚未被調度完成，或者仍處於從倉庫下載 image 的過程中 Running
POD 已經被調度到某節點，且所有容器已由 kubelet 創建 Failed</description></item><item><title>kubernetes - day09</title><link>https://cch0124.github.io/kubernetes/2020-08-30-label-selector/</link><pubDate>Sun, 30 Aug 2020 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/kubernetes/2020-08-30-label-selector/</guid><description>這一章節承接第 8 天的文章，此篇文章要分享的是標籤和標籤選擇器的應用。
在一個 Kubernetes 環境下，當 POD 數量越來越多，分類和管理將會變得重要，因為這樣才能提升管理的效率，而在 K8s 中有一個 Label 的字段，它可用來定義資源而外的訊息像是測試環境、開發環境、前端等，往後在藉由標籤選擇器進行過濾完成想要的目標和任務。
標籤 標籤可以依附在每個 K8s 資源對象之上，一個對象可有不止一個標籤，而同一個標籤可被新增到多個資源上。如上述所講，它可以靈活的進行資源對象的分類進行管理，標籤可用版本、環境、分層架構等方式進行貼標籤動作。下圖為 &amp;ldquo;Kubernetes in action&amp;rdquo; 書中圖片
相較於 annotations，annotations 無法用來挑選資源對象，僅提供&amp;quot;原數據&amp;quot;，類似註解
管理標籤 K8s 中標籤字段定義在 metadata 上，以下面為範例，定義兩個標籤分別是 app 和 tier，值分別是 myapp 和 backend。
# pod-demo.yaml apiVersion: v1 kind: Pod metadata: name: pod-label-demo namespace: default labels: app: myapp tier: backend spec: containers: - name: myapp image: nginx:1.18 如果上面的 yaml 檔部署完成後，可用 --show-labels 的選項，讓列出 POD 資源時帶有定義的標籤。
$ kubectl get pod --show-labels NAME READY STATUS RESTARTS AGE LABELS pod-label-demo 1/1 Running 0 10s app=myapp,tier=backend 假設標籤給很多時，使用 -L key1,key2.</description></item><item><title>kubernetes - day08</title><link>https://cch0124.github.io/kubernetes/2020-08-29-management-pod/</link><pubDate>Sat, 29 Aug 2020 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/kubernetes/2020-08-29-management-pod/</guid><description>POD 和容器一樣，應該只運行一個應用，這樣才有輕量化的感覺。舉例來說，前端和後端應該要在各自的 POD 上，這樣的優勢有很多，像是被調度到不同節點上運行，提高資源使用上的效率。然而，Kubernetes 的伸縮功能，可針對每個獨立的 POD 進行，這樣提高了靈活性。但是，實際上有些系統設計需要在一個 POD 中運行多個容器，而這些的設計想必又會有一套原則去實踐，如下：
Sidecar pattern Ambassador pattern Adapter pattern 等等 管理 POD 容器 apiVersion: v1 kind: Pod metadata: name: pod-demo namespace: default labels: app: myapp tier: frontend spec: containers: - name: myapp # 容器名稱 image: nginx:1.18 # image - name: busybox image: busybox:latest command: [&amp;#34;bin/sh&amp;#34;, &amp;#34;-c&amp;#34;, &amp;#34;sleep 3600&amp;#34;] 以上面的 POD 資源清單來看，containers 是被用來定義容器清單。在 POD 中必定要有一個容器，因此該字段必須是要定義的。然而容器的環境設置還有許多參數可設定，可用以下方式去查看，會列出關於 containers 的相關字段。
$ kubectl explain pods.spec.containers KIND: Pod VERSION: v1 RESOURCE: containers &amp;lt;[]Object&amp;gt; .</description></item><item><title>kubernetes - day07</title><link>https://cch0124.github.io/kubernetes/2020-08-28-namespace-pod-resource-operation/</link><pubDate>Fri, 28 Aug 2020 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/kubernetes/2020-08-28-namespace-pod-resource-operation/</guid><description>查看 Namespace 與資源對象 預設的 Kubernetes 提供了幾個 namespace 用於不同的目的，下面的結果在 GKE 或 kubeadm 上目前使用是一樣的，至於 namespace 名稱的用途可參考此鏈接
$ kubectl get namespace NAME STATUS AGE default Active 13h kube-node-lease Active 13h kube-public Active 13h kube-system Active 13h 針對某一個特定 namespace 進行詳細訊息查看
$ kubectl describe namespaces default 如果使用了 namespace 將一些應用進行隔離，我們要查看特定 namespace 下資源時須使用 -n 參數進行切換，預設是在 default 上。下面結果是 GKE 的環境。
$ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE event-exporter-v0.3.0-5cd6ccb7f7-mp7p4 2/2 Running 0 13h fluentd-gcp-scaler-6855f55bcc-mchvv 1/1 Running 0 13h fluentd-gcp-v3.</description></item><item><title>kubernetes - day06</title><link>https://cch0124.github.io/kubernetes/2020-08-27-resource-format/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/kubernetes/2020-08-27-resource-format/</guid><description>資源配置清單 Kubernetes 中資源基本上都是需要五個字段定義資源，分別是 apiVersion、kind、metadata、spec 和 status。以下分別介紹
apiVersion group/version 定義 api-versions kind 資源類型 metadata 用來定義一些訊息，如名稱、所屬 namespace 與標籤等 spec 定義所期望的狀態 status 紀錄當前對象狀態 由 Kubernetes 維護，因此客戶端不需要去定義 這邊使用 get 方式取得 kube-system 的預設 namespcae 資訊，並以 yaml 輸出。
$ kubectl get namespace kube-system -o yaml apiVersion: v1 kind: Namespace metadata: creationTimestamp: &amp;#34;2020-08-22T02:00:45Z&amp;#34; name: kube-system resourceVersion: &amp;#34;4&amp;#34; selfLink: /api/v1/namespaces/kube-system uid: 73058111-fb1f-48a6-b283-338207167625 spec: finalizers: - kubernetes status: phase: Active 大致上相關資源都是以上面格式進行清單定義創建資源。以下練習建立一個 namespace 資源。為何不用定義跟上述一樣的字段 ?</description></item><item><title>kubernetes - day05</title><link>https://cch0124.github.io/kubernetes/2020-08-26-pod-controller-resource/</link><pubDate>Wed, 26 Aug 2020 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/kubernetes/2020-08-26-pod-controller-resource/</guid><description>上一篇文章，教大家操作 Kubernetes 群集相關的操作，之前也大致說明 POD 是什麼。今天這篇文章會介紹到與 POD 相關的 POD 控制器和一些資源應用，這為了能夠更加豐富的運用 POD 資源，進而將容器的應用變得更加靈活、完善和操作等。下圖顯示了 Kubernetes POD 如何被 Kubernetes 的更抽象資源去應用。
在前面文章有介紹過 POD，它負責運行容器，同時解決環境依賴的一些問題，可能會是共享的儲存、配置資訊或網頁伺服器要的 SSL 等。然而 POD 的運行過程中可能會遇到一些資源配置不足或是一些突發狀況導致 POD 非正常的終止。這問題 Kubernetes 由負載類型的 POD 控制器去負責，適當將故障的 POD 重建。
雖然 POD 控制器能夠重建資源，但有些應用程序是有順序的概念，因此這邊又可將 POD 控制器分類為無狀態和有狀態類型。圖中 ReplicaSet 和 Deployment 屬於管理無狀態類型服務，StatefulSet 則是有狀態。除了這些， Kubernetes 還提供一些特殊應用的服務，像是 DaemonSet 它用於為每個節點上運行單一個 POD 的資源，可能是 Log 蒐集或是監控服務等應用，還有 Job 控制器，它可以提供短暫批次任務，完成後即可終止該資源。
POD 資源一般來說是群集內可相互存取的，再次重建後也應該要被發現。如果 POD 要給外部使用者存取則需要透過暴露方式，並且要有負載均衡。在 Kubernetes 中透過 Service 和 Endpoint 與 Ingress 可以解決發現、服務暴露和附載均衡。
在 Kubernetes 中設計了 Volume，可用於幫助 POD 的資源儲存，它支援了儲存設備和系統，像是 GlusterFS、CEPH RBD 和 NFS 等。不過 Kubernetes 有抽象了 CSI（Container Storage Interface） 統一儲存介面以便擴展更多的儲存應用。容器在運行時將不同環境所需的變數用環境變數作為解決方案，但容器啟動後不得更改。在 Kubernetes 中 configMap 能夠以環境變數或volume方式傳入至 POD 的容器中，它同時可被不同應用的 POD 共享，使得靈活性提高，然而對於此方式較隱密的資訊較不適合 configMap 需使用 Secret。</description></item><item><title>kubernetes - day04</title><link>https://cch0124.github.io/kubernetes/2020-08-25-k8s-bubectl-basic-operation/</link><pubDate>Tue, 25 Aug 2020 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/kubernetes/2020-08-25-k8s-bubectl-basic-operation/</guid><description>操作環境有 GKE 與 kubeadm 自架群集操作，本篇文章將會帶讀者學會使用 kubectl 觀看集群相關的資訊。
查看預設安裝的 POD 資源 GKE 的環境
$ kubectl get pods -n kube-system # -n 表示 namespace，kube-system 為指定的 namespace NAME READY STATUS RESTARTS AGE event-exporter-v0.3.0-5cd6ccb7f7-mp7p4 2/2 Running 0 13h fluentd-gcp-scaler-6855f55bcc-mchvv 1/1 Running 0 13h fluentd-gcp-v3.1.1-f8wc8 2/2 Running 0 13h fluentd-gcp-v3.1.1-g6mbn 2/2 Running 0 13h fluentd-gcp-v3.1.1-zq4xm 2/2 Running 0 13h heapster-gke-7c7bdf567c-cmqhm 3/3 Running 0 13h kube-dns-5c446b66bd-5ltbw 4/4 Running 0 13h kube-dns-5c446b66bd-fqvwk 4/4 Running 0 13h kube-dns-autoscaler-6b7f784798-hr8ck 1/1 Running 0 13h kube-proxy-gke-cluster-1-test-default-pool-255d7fb2-1f8l 1/1 Running 0 13h kube-proxy-gke-cluster-1-test-default-pool-255d7fb2-lbwc 1/1 Running 0 13h kube-proxy-gke-cluster-1-test-default-pool-255d7fb2-ppnm 1/1 Running 0 13h l7-default-backend-84c9fcfbb-kwrrs 1/1 Running 0 13h metrics-server-v0.</description></item><item><title>kubernetes - day03</title><link>https://cch0124.github.io/kubernetes/2020-08-24-kubernetes-pod/</link><pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/kubernetes/2020-08-24-kubernetes-pod/</guid><description>Kubernetes 核心資源 POD 介紹 在集群的 Master 上提供了 API Server 元件，同時有著 RESTful 的風格。像是 POD 資源包含了所有 POD 對象的集合，其用於描述在哪個節點進行容器的實例、需要配置什麼樣的環境(硬體資源或儲存)和管理上的策略，其策略可能是重啟、升級等。另外的 Kubernetes 也提供比 POD 更抽象的 POD 控制器，可用來確保 POD 的存在，這些控制器有 Deployment、Service 等。
POD 資源對象 POD 是一到多個容器應用的集合，這也包含著儲存、網路等元件。POD 可以想成是一個應用程式運行的實例，當中共享著前面提到的原件，如下圖所示。
因為共享著網路，所以 POD 中的對象都會是同一網段。因此可以透過 IP 直接進行通訊，不論是在哪一個節點上。所以這樣的結構可以想像成是一個 VM 虛擬機，但不同的是 POD 中的對象容器各個行程都相互彼此隔離。整體來看，該 POD 中的網路與儲存是容器的關鍵資源。但是，與 POD 外部的元件通訊需要透過 service 資源所創建的 ClusterIP 等類型和對應 Port 進行通訊。為了讓 POD 中資源能夠共享數據，因此提供了 volume 的資源。在更高階的抽象中該 volume 還可以讓容器在生命週期中時確保數據不遺失。
之後的章節會分享對於 POD 的擴充機制、訊息的處裡等資源。
圖中我們還看到一個為 Pause 的容器，該容器是 POD 中的基礎設施。它的 image 很小，都處於暫停狀態，它是解決 POD 網路而產生的，所以 POD 的 Network Namespace 的對應就是該容器。其實現程式碼從此鏈接可以得知如何實現。</description></item><item><title>kubernetes 1.18 安裝-day 02</title><link>https://cch0124.github.io/kubernetes/2020-08-20-kubernetes-install/</link><pubDate>Thu, 20 Aug 2020 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/kubernetes/2020-08-20-kubernetes-install/</guid><description>Introduction k8s 是一個用來編排容器的工具，這近幾年非常火紅。有著高可靠的架構、應用程序部署、自動部署、擴展等功能。
本篇文章已安裝為目的。
Environment 實驗環境會有一台 master 和兩台 node 而資源配置如下
CPU 2 vCPU memory 2 GB 192.168.134.131 | master 192.168.134.133 | node01 192.168.134.135 | node02 Configure Hostname $ sudo hostnamectl set-hostname master $ sudo hostnamectl set-hostname node01 $ sudo hostnamectl set-hostname node02 設定完之後，會出現 DNS 問題。此問題會讓本機無法解析，如下解決
$ sudo vim /etc/hosts 192.168.134.131 master 192.168.134.133 node01 192.168.134.133 node02 Disable Swap 所有機器應該要固定 CPU/memory，不應該使用 swap 會導致效能降低。
$ sudo swapoff -a $ sudo swapon -s 要保持永久效果至 fstab 將 swap 項目註解</description></item><item><title>K8s-day01</title><link>https://cch0124.github.io/kubernetes/2020-08-19-kubernetes-overview/</link><pubDate>Wed, 19 Aug 2020 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/kubernetes/2020-08-19-kubernetes-overview/</guid><description>什麼是 Kubernetes Kubernetes 是一個集群管理系統，它擁有可移植、擴展和高可用等特性。它用於管理容器(container)，然而容器的出現改變了佈署方式。容器類似於 VM 與主機共享內核(kernel)，容器有自己的檔案系統(filesystem)、CPU、記憶體(memory)、進程空間(process space)等。因為它們與基礎架構分離，因此可以跨雲(公、私有或混合雲中)和作業系統進行移植。然而容器佈署的數量越多管理難度明顯的會提升，使用者必須對容器進行分組，並對其分配網路、監控或安全等服務。藉由 kubernetes 提供的編排和管理功能，以便完成大規模佈署，且 kubernetes 提供了資源調度、容器擴展等管理功能，這些的作用讓 kubernetes 可以管理容器應用程式的生命週期。
Kubernetes 架構 Kubernetes 匯集多個物理機、虛擬機將其變成一個集群。如下圖
master 是用戶端與集群之間的核心聯絡重要位置 追蹤其它服務器的健康狀態 以最佳方式調度工作的負載 編排其它組件之間的任務 node 集群中的工作節點，負責接收來自 master 的工作調度 創建或銷毀 POD 調整網路好讓路由與轉發流量 POD Kubernetes 中嘴小運算單元 封裝多個容器 一個 POD 中的容器共享 namespace 和儲存資源，但彼此又在 mount、user、PID 等名稱空間保持隔離 在 kubernetes 中有很多的組件會來支撐整個集群的運作邏輯，像是編排、暴露、恢復等，之後文章會慢慢的提到。
master 組件 API Server 負責輸出 RESTful 的 Kubernetes API，負責接收和驗證相關的請求，其狀態資源會被存入 etcd 中。
etcd 該 etcd 會確保生產環境的可用性。它會提供監聽的機制，用於推送和監聽變更。只要有發生變化便會與 API Server 溝通，並做出適當的動作向客戶端輸出。</description></item><item><title>kubernetes</title><link>https://cch0124.github.io/kubernetes/2019-09-20-kubernetes/</link><pubDate>Fri, 20 Sep 2019 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/kubernetes/2019-09-20-kubernetes/</guid><description>Introduction k8s 是一個用來編排容器的工具，這近幾年非常火紅。有著高可靠的架構、應用程序部署、自動部署、擴展等功能。
本篇文章已安裝為目的。
Kubernetes Cluster Components Node 被 Master 管理的節點可以是實體機或虛擬機 每個 Node 上執行 Pod 服務，並進行管理和啟動透過 kubelet Pod kubernetes 中運行的最小單位，可以是一個或多個容器 透過 Node 新增、刪除、啟動 生命週期存在 4 種狀態（Pending、Running、Succeeded、Failed） 具有共享 volumn，提供對 Pod 中所有容器的存取 Selector 透過比對物件針對任何標籤進行的查詢 Replication Controller 定義 Pod 數量 在 master 上，Controller Manager 透過 RC 的定義完成 Pod 建立、監控、啟動、停止等操作 任何 Pod 發生故障，它會讓群集狀態恢復正常 Label 定義物件的可辨識屬性，用它們進行管理和選擇 key-value 定義 Service 用來提供 Pod 對外存取的介面 NodePort LoadBlance Ingress by yeasy.</description></item></channel></rss>