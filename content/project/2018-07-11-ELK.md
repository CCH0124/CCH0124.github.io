---
title: ELK
data: 2018-07-11
description: "建立 ELK 基本環境"
tags: [ELK]
draft: false
---

# ELK

ELK 是指 ElasticSearch 、 Logstash 和 Kibana 三個 open-source 的集合套件，這三個軟體可以組成一套日誌(log)分析架構。

如今資料量動輒 "T" 或幾個 "P" 等級的數據，正常的文字編輯器或數據處理軟體等工具往往都難以應對，但對原始的大數據進行分析，能提高對數據的洞察力。ELK 可以對大量Log 的數據處理，如索引、分析，當對這大量進行良好的控管，將可以依照這些數據得到一些有用的資訊。

## Log 

我是應用在 Log 的蒐集上因此小提一下 Log。

Log 就是系統或設備在連線和運作時所產生的記錄，藉由 Log 的蒐集和分析，讓 IT 人員能夠監控系統的運作狀態，判斷可能發生的事件，以及分析資料存取行為和使用者的活動。

網路是否遭到惡意的嘗試入侵？
系統和網路運作是否有異常情況發生？
IT 人員只要對 Log 進行監控，就可以判斷可能的問題，或著狀況不清楚時，透過 Log 的查詢分析，可在較短時間內找出原因。

## 架構

![](https://i.imgur.com/jM8xjRv.png)

## 核心架構介紹

### Elasticsearch

- 全文檢索的搜索引擎
- 擴展性高
- 分散式系統的功能
- 索引方式管理維護數據
- 透過 API 取得相關數據的查詢、聚合等

### Logstash

- 分析數據的入口點
- 支持很多的 Input、Output 數據套件
- 其中一種是將數據傳入至 Elasticsearch
- 蒐集原始數據
- 修改（過濾）數據，並將其數據轉換成有意義的資訊
- 完成數據格式化和重新組織數據

#### 資料進入 logstash 流程

![](https://i.imgur.com/GdAxDbg.png)

- input
    - file
    - TCP
    - UDP
    - syslog
    - beat
    - 等等
- filter
    - grok
    - mutate
    - drop
    - 等等
- output
    - Elasticsearch
    - google_bigquery
    - 等等


其中 Data Source 可以是 beat 或讀檔等方式傳入 Logstash，在 filter 部分可以處裡傳入的數據，最橫在把資訊傳至 Elasticsearch。

### Kibana

- 圖表的可是視化

![](https://i.imgur.com/ZY2NWm8.png)

#### GUI 

- 黑框
    - Discover：用以檢視各索引下的記錄內容及總記錄筆數
    - Visualize：將搜尋出的數據以長條圖、圓餅圖、表格等方式呈現
    - Dashboard：將以儲存的搜尋結果或已完成的圖表組合成一份快速報表
    - Timelion：時序性的監看 query
    - Dev Tools：提供一個在 kibana 直接呼叫 elasticsearch 的方式
    - Managment：設定 kibana 對應的 elasticsearch index patterns，管理已經儲存好的搜尋結果物件、視覺化結果物件，及進階資料過濾設定
- 紅框
    - Index：要搜索的 index pattern（Management 所設定的 index patterns create ）
- 咖啡框
    - Avaliable Fields：搜索 index 下所包含的屬性(也就是我們在 logstash 切出來的部分)
- 紫框
    - Timestamp：資料時間，可用於特定時間區間資料量觀察
- 藍框
    - source：顯示我們接收到的 log 資訊
- 綠框
    - 搜尋條件：預設 "*" 搜尋 index 下所有紀錄

## 實作
### 環境

- ubuntu
    - Memory
        - 2G
    - Docker
    - Docker-compose

### 安裝環境

##### Docker

```shell=
$ sudo apt-get update
$ sudo apt-get install linux-image-extra-$(uname -r) linux-image-extra-virtual
$ sudo apt-get update
$ sudo apt-get install apt-transport-https ca-certificates curl software-properties-common
$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
$ sudo apt-key fingerprint 0EBFCD88
$ sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
$ sudo apt-get update
$ sudo apt-get install docker-ce
```
##### Docker-compose

```shell=
$ sudo curl -L https://github.com/docker/compose/releases/download/1.21.0/docker-compose-$(uname -s)-$(uname -m) -o /usr/local/bin/docker-compose
$ sudo chmod +x /usr/local/bin/docker-compose
```
### 定義 ELK Docker-compose

```shell=
version: '3'

services:

  elasticsearch:
    build:
      context: elasticsearch/
      args:
        ELK_VERSION: $ELK_VERSION
    container_name: elasticsearch
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - ./elasticsearch/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml:ro
    ports:
      - "9200:9200"
      - "9300:9300"
    environment:
      ES_JAVA_OPTS: "-Xmx1024m -Xms1024m"
    networks:
      - elk

  logstash:
    build:
      context: logstash/
      args:
        ELK_VERSION: $ELK_VERSION
    container_name: logstash
    volumes:
      - ./logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml:ro
      - ./logstash/config/pipelines.yml:/usr/share/logstash/config/pipelines.yml:ro
      - ./logstash/pipeline:/usr/share/logstash/pipeline:ro
      - ./logstash/DataSet:/usr/share/logstash/DataSet:ro
    ports:
      - "5001:5001"
      - "9600:9600"
    environment:
      LS_JAVA_OPTS: "-Xmx256m -Xms256m"
    networks:
      - elk
    depends_on:
      - elasticsearch

  kibana:
    build:
      context: kibana/
      args:
        ELK_VERSION: $ELK_VERSION
    container_name: kibana
    volumes:
      - ./kibana/config/:/usr/share/kibana/config:ro
    ports:
      - "5601:5601"
    networks:
      - elk
    depends_on:
      - elasticsearch

networks:
  elk:
    driver: bridge

```

關於 docker-compose 可參考 [本人 Docker-elk](https://github.com/CCH0124/docker-elk)，會更上述的 docker-compose 不一樣，本人有新增其它服務。

>$ELK_VERSION 是從 .env 定義。可參考[environment-variables](https://docs.docker.com/compose/environment-variables/)

#### Run

進入 docker-compose.yml 檔案該層目錄執行以下。否則需要使用 -f 來指定 docker-compose.yml 位置。

```shell=
$ sudo docker-compose up -d --build
```
#### ELK Port

- 5001：Logstash TCP input
- 9200：Elasticsearch HTTP
- 9300：Elasticsearch TCP transport
- 5601：Kibana

#### Kibana 瀏覽

```shell=
http://localhost:5601
```

![](https://i.imgur.com/BSNRA0A.png)


#### 測試 logstash TCP input

```shell=
$ nc localhost 5001 < a.txt
```
檢查 kibana 上是否有索引出現

![](https://i.imgur.com/9lbBhaz.png)

接著建立索引，按下 "Next step"

![](https://i.imgur.com/kNMOOiD.png)

選擇時間戳，按下 "Create index pattern"

![](https://i.imgur.com/M2jH1fU.png)

點選 Discover 即可看見，藉由 logstash 傳到 elasticsearch，kibana 再從 elastcsearch 抓出資料顯示。 

![](https://i.imgur.com/pIvnNSN.png)

## 結論

以上建立好基本上資料傳遞都會成功，之後會介紹 `logstahs` 讀檔以及一些 beat 的應用。目前還會研究 Elasticsearch 的叢集和 logstash 結合 redis 軟體這些叫細節的規劃。

其實 ELK 這開源的軟體，我是透過電算老師介紹，當初我是利用 `rsyslog` 來進行 log 蒐集，後來發現覺得好不人性化。
因此，就像恩人請求辦法，最後也認識了 ELK 三個開源的軟體。 

