<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ELK on</title><link>https://cch0124.github.io/tags/elk/</link><description>Recent content in ELK on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 12 Jun 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://cch0124.github.io/tags/elk/index.xml" rel="self" type="application/rss+xml"/><item><title>Elasticsearch 筆記</title><link>https://cch0124.github.io/blog/2020-06-12-es/</link><pubDate>Fri, 12 Jun 2020 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/blog/2020-06-12-es/</guid><description>特點
分散式的即時文件儲存，每個字段都可被索引並可搜索 分散式即時分析搜索引擎 不規則查詢 高擴展，可處裡 PB 級結構或非結構化數據 Lucene 實現索引和搜索功能 透過簡單的 RESTful API 來隱藏 Lucene 的複雜性，讓搜索變簡單
ES 能做什麼 全文檢索 模糊查詢 數據分析 聚合等 Elasticsearch 的交互方式 基於 HTTP 協定，以 JSON 為數據交互格式的 RESTful API _cat $ curl &amp;#39;http://192.168.227.141:9200/_cat&amp;#39; ^.^ /_cat/allocation /_cat/shards /_cat/shards/{index} /_cat/master /_cat/nodes /_cat/tasks /_cat/indices /_cat/indices/{index} /_cat/segments /_cat/segments/{index} /_cat/count /_cat/count/{index} /_cat/recovery /_cat/recovery/{index} /_cat/health /_cat/pending_tasks /_cat/aliases /_cat/aliases/{alias} /_cat/thread_pool /_cat/thread_pool/{thread_pools} /_cat/plugins /_cat/fielddata /_cat/fielddata/{fields} /_cat/nodeattrs /_cat/repositories /_cat/snapshots/{repository} /_cat/templates $ curl &amp;#39;http://192.</description></item><item><title>input csv to elasticsearch</title><link>https://cch0124.github.io/project/2019-11-15-elk-input-csv/</link><pubDate>Fri, 15 Nov 2019 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/project/2019-11-15-elk-input-csv/</guid><description>想法 實驗透過 logstash 傳遞 CSV 檔至 Elasticsearch。這邊 CSV 檔是來自於這邊的數據集，因為很懶得用 python 來分析資料，且該數據集太肥大，所以想藉由 ELK 了力量 XD。
目錄架構
$ tree . ├── docker-compose.yml ├── elasticsearch │ ├── config │ │ ├── elasticsearch-node2.yml │ │ ├── elasticsearch-node3.yml │ │ └── elasticsearch.yml │ └── Dockerfile ├── kibana │ ├── config │ │ ├── kibana.crt │ │ ├── kibana.key │ │ └── kibana.yml │ └── Dockerfile ├── logstash │ ├── config │ │ ├── logstash.yml │ │ └── pipelines.</description></item><item><title>logstash Multiple Pipelines</title><link>https://cch0124.github.io/project/2018-12-22-logstash-multiple-pipelines/</link><pubDate>Sat, 22 Dec 2018 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/project/2018-12-22-logstash-multiple-pipelines/</guid><description>logstash Multiple Pipelines 遇到的問題是，使用 filebeat 將 json 數據傳遞給 logstash 處裡並建立索引 pcap 與使用 logstash 讀取 CSV 數據建立 CSV 索引。 兩者都是不同的索引。但是 kibana 上的 pcap 索引卻讀取 csv 索引數據。這造成分析上的錯誤。以下會先了解 logstash 運作以及解決方式。
Logstash 事件處理管道有三個階段：inputs → filters → outputs。
inputs 生成事件 filters 修改它們，輸出將它們發送到其他地方 輸入和輸出支援編解碼，能夠在數據進入或退出管道時對數據進行編碼或解碼，而無需使用單獨的 filters。
inputs 將輸入的數據導入至 logstash。 常用的輸入：
file 從文件系統上的文件讀取，與UNIX命令非常相似 tail -0F syslog 在已知端口514上偵聽syslog消息並根據RFC3164格式進行解析 redis 使用 redis 通道和 redis 列表從 redis 服務器讀取。 Redis 通常用作集中式 Logstash 安裝中的&amp;quot;broker&amp;quot;，該安裝將 Logstash 事件從遠程 Logstash &amp;ldquo;shippers&amp;rdquo; 排隊。 beats 處理 Beats發送的事件 plugins</description></item><item><title>ELK</title><link>https://cch0124.github.io/project/2018-07-11-elk/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/project/2018-07-11-elk/</guid><description>ELK ELK 是指 ElasticSearch 、 Logstash 和 Kibana 三個 open-source 的集合套件，這三個軟體可以組成一套日誌(log)分析架構。
如今資料量動輒 &amp;ldquo;T&amp;rdquo; 或幾個 &amp;ldquo;P&amp;rdquo; 等級的數據，正常的文字編輯器或數據處理軟體等工具往往都難以應對，但對原始的大數據進行分析，能提高對數據的洞察力。ELK 可以對大量Log 的數據處理，如索引、分析，當對這大量進行良好的控管，將可以依照這些數據得到一些有用的資訊。
Log 我是應用在 Log 的蒐集上因此小提一下 Log。
Log 就是系統或設備在連線和運作時所產生的記錄，藉由 Log 的蒐集和分析，讓 IT 人員能夠監控系統的運作狀態，判斷可能發生的事件，以及分析資料存取行為和使用者的活動。
網路是否遭到惡意的嘗試入侵？ 系統和網路運作是否有異常情況發生？ IT 人員只要對 Log 進行監控，就可以判斷可能的問題，或著狀況不清楚時，透過 Log 的查詢分析，可在較短時間內找出原因。
架構 核心架構介紹 Elasticsearch 全文檢索的搜索引擎 擴展性高 分散式系統的功能 索引方式管理維護數據 透過 API 取得相關數據的查詢、聚合等 Logstash 分析數據的入口點 支持很多的 Input、Output 數據套件 其中一種是將數據傳入至 Elasticsearch 蒐集原始數據 修改（過濾）數據，並將其數據轉換成有意義的資訊 完成數據格式化和重新組織數據 資料進入 logstash 流程 input file TCP UDP syslog beat 等等 filter grok mutate drop 等等 output Elasticsearch google_bigquery 等等 其中 Data Source 可以是 beat 或讀檔等方式傳入 Logstash，在 filter 部分可以處裡傳入的數據，最橫在把資訊傳至 Elasticsearch。</description></item></channel></rss>