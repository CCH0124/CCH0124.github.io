<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>logstash on</title><link>https://cch0124.github.io/tags/logstash/</link><description>Recent content in logstash on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sat, 22 Dec 2018 00:00:00 +0000</lastBuildDate><atom:link href="https://cch0124.github.io/tags/logstash/index.xml" rel="self" type="application/rss+xml"/><item><title>logstash Multiple Pipelines</title><link>https://cch0124.github.io/project/2018-12-22-logstash-multiple-pipelines/</link><pubDate>Sat, 22 Dec 2018 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/project/2018-12-22-logstash-multiple-pipelines/</guid><description>logstash Multiple Pipelines 遇到的問題是，使用 filebeat 將 json 數據傳遞給 logstash 處裡並建立索引 pcap 與使用 logstash 讀取 CSV 數據建立 CSV 索引。 兩者都是不同的索引。但是 kibana 上的 pcap 索引卻讀取 csv 索引數據。這造成分析上的錯誤。以下會先了解 logstash 運作以及解決方式。
Logstash 事件處理管道有三個階段：inputs → filters → outputs。
inputs 生成事件 filters 修改它們，輸出將它們發送到其他地方 輸入和輸出支援編解碼，能夠在數據進入或退出管道時對數據進行編碼或解碼，而無需使用單獨的 filters。
inputs 將輸入的數據導入至 logstash。 常用的輸入：
file 從文件系統上的文件讀取，與UNIX命令非常相似 tail -0F syslog 在已知端口514上偵聽syslog消息並根據RFC3164格式進行解析 redis 使用 redis 通道和 redis 列表從 redis 服務器讀取。 Redis 通常用作集中式 Logstash 安裝中的&amp;quot;broker&amp;quot;，該安裝將 Logstash 事件從遠程 Logstash &amp;ldquo;shippers&amp;rdquo; 排隊。 beats 處理 Beats發送的事件 plugins</description></item></channel></rss>