<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Machine Learning on</title><link>https://cch0124.github.io/tags/machine-learning/</link><description>Recent content in Machine Learning on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sat, 29 Aug 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://cch0124.github.io/tags/machine-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>30 天學習歷程-day11</title><link>https://cch0124.github.io/blog/2020-08-29-feature-selection/</link><pubDate>Sat, 29 Aug 2020 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/blog/2020-08-29-feature-selection/</guid><description>特徵選取是指在開發一個機器學習模型時，減少輸入特徵數量的過程。這過程不但能減少計算上的成本，有時還能因為特徵選取減少了聲噪的影響因而建構出一個良好的模型。特徵選擇可分為以下
Unsupervised 移除多餘的特徵 Correlation Supervised 移除無關連特徵 Wrapper RFE Filter 依照特徵集合和目標的關係選擇特徵集合 Statistical 方法 SelectKBest SelectPercentil Feature Importance 方法 Intrinsic 訓練過程中執行自動特徵選取的演算法 Decision Tree Dimensionality Reduction 將數據投影到低維度的特徵空間中 統計的特徵選取方法 通常在輸入和輸出變量之間使用 correlation 統計作為過濾器特徵選擇的基礎。統計量測選擇高度依賴於可變數據類型，如下
數值 Integer Floating 分類 Boolean Ordinal Nominal 從數據類型來看的話數值是屬於 Regression 問題，分類是 Classification 問題。通常過濾器特徵選擇中使用的統計測量與目標變數一次計算一個輸入變數。因此，它們被稱為單變量統計(univariate statistical)測量。</description></item><item><title>30 天學習歷程-day10</title><link>https://cch0124.github.io/blog/2020-08-28-clustering/</link><pubDate>Fri, 28 Aug 2020 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/blog/2020-08-28-clustering/</guid><description>分群(clustering) 是將一堆資料物件聚類成數個群集，讓同一群擊內的資料物件有很高的相似性，而不同群集間則有不相似的特性。在判別資料上的相似性會依照資料特徵進行衡量，通常可能會是量測距離等。
群集分析是什麼 就是將觀測的資料切分不同子集合的動作，每一個子集合都是一個群集。在百百種的分群演算法中，每種形成群集的效果都是不一樣，這些的演算法很適合挖掘未知的資訊。群集分析可以用來挖掘資料內部的分布，觀察每個群集的特徵，並進行下一步地分析動作。當然也可用作於資料前處理步驟，像是資料特性、屬性子集合選取或分類法等。而分群相較於分類，它能夠自動的找到群組。
集群分析方法 分割式分群法 找出互斥的球形群集 以距離為基礎 使用 mean 與 medoid 來代表群集中心點 對於中小型資料集很有效率 演算法有，k-means、k-medoids 等
階層式分群法 透過階層分解方式來分群 不能修正錯誤的合併或分割 可以結合微分群技術或考慮資料物件間的關聯性 演算法有，BIRCH、Chameleon 等
密度式分群法 找到任意形狀的群集 群集為空間中的資料物件密集的區域，不同群集則被低密度區域分隔 群集密度 每個資料物件得鄰近區域內包含至少最小數量的資料點 可以過濾離群值 演算法有，DBSCAN、OPTICS、DENCLUE 等</description></item><item><title>30 天學習歷程-day09</title><link>https://cch0124.github.io/blog/2020-08-27-what-classification/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/blog/2020-08-27-what-classification/</guid><description>以棒球員捕手為例，當他面對一位打者時，需要判斷說該打者對於什麼球種是安全的，那些球種是有風險的；醫療方面可能是分析乳癌的病患資料，並在幾種特定解決方法中，該給於怎樣的治療。這些例子中，資料分析方法就是分類(classification)。假設是預測棒球雙方的比數，該資料分析任務就是數值預測，迴歸分析(regression analysis)是最常用來數值預測的統計方法。
要進行資料分類，其程序大致為兩步驟，學習步驟此階段會建立模型；分類步驟此階段會利用模型來預測給定資料的類別標籤。學習步驟中會給定資料集合中的資訊，藉由分類演算法分析資料集合中一組樣本和其對應的類別標籤建立一個分類器。一個值組 $X$ 為 $n$ 維度的特徵向量，$X = (x_1, x_2, &amp;hellip;, x_3)$，其每個 $X$ 會對應一個類別，該類別可由類別標籤屬性來定義。假設一個樣本對應的標籤已經被定義好，該學習步驟可稱為監督式學習(supervised learning)，這與**非監督式學習(unsupervised learning)**不同，樣本無對應的標籤，需透過學習才知道，常見的方式就是使用分群(clustering)。以之前寫過的簡單線性迴歸來說，我們會希望透過映射函數來預測給定的資料樣本 $X$ 對應的類別標籤 $y$，就是找一個函數來分割資料的類別。
分類步驟會使用模型來進行分類，然而其正確率是多少 ? 在計算該值時不應該拿訓練資料進行評估，因分類器會傾向於過擬合(overfit) 訓練資料，就是說學習過程中，有些異常的資料會被過度學習，而該異常資料並不會出現在一般的資料集中。我們應當使用測試資料來進行正確率的評分。</description></item><item><title>30 天學習歷程-day08</title><link>https://cch0124.github.io/blog/2020-08-26-math/</link><pubDate>Wed, 26 Aug 2020 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/blog/2020-08-26-math/</guid><description>二項式定理、指數、對數、三角函數 二項是定理 參考 參考 排列組合 排列
由 $n$ 個不同物品取 $k$ 個出來排列，因此最後的排列順序不同，及視為不同的排列 組合
由 $n$ 個不同物品取 $k$ 個出來但不排列，因此只要組成元素相同，及視為相同組合，無關順序排列 ABC、ACB、BAC、CAB、BCA、CBA 這 $3!$ 種排列可看成是 A、B、C 的組合 組合數可用 $\frac{n!}{(n-k)!k!}$ 或 $C^n_k$ 表示 二項分佈計算獨立事件的機率分佈 事件的成功機率與獨立性 拜訪 10 次簽下 $k$ 建的機率一般式 $0.36$ 每次拜訪成功機率，
$P(x=k) = \tbinom{10}{k} \centerdot 0.36^k \centerdot (1-0.36)^{10-k}$
拜訪 10 次簽下 2 件的機率 $P(x=2) = \tbinom{10}{2} \centerdot 0.36^2 \centerdot (1-0.36)^{10-2} = \frac{10!}{(10-2)!2!} \centerdot 0.</description></item><item><title>30 天學習歷程-day07</title><link>https://cch0124.github.io/blog/2020-08-25-simple-linear-regression/</link><pubDate>Tue, 25 Aug 2020 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/blog/2020-08-25-simple-linear-regression/</guid><description>回歸模型（線性或非線性）被廣泛應用於數值預測，比如薪水，銷售額等等。如果說自變數（independent variable）是時間，那麼我們是在預測未來的數值；反之我們的模型在預測當前未知的數值。
簡單線性回歸，由自變量 $X$ 來預測因變量 $Y$ 的方法，假設這兩個變量是相關的線性，可嘗試尋找依據特徵($x$)的線性函數來擬合並預測($y$)。
from wiki
上圖中，紅線可以用 $y=ax+b$ 求得，且該紅線是能代表該資料的一條線，其 $a$ 為斜率；$b$ 為y截距。$y$ 為應變數 ，$x$ 為自變數。然而為了找到最佳擬合的線，會使最小平方法，該方法盡可能的讓預測值與實際值的誤差為最小。其公式如下，並對應下圖，$y_i$ 為實際值；$y_p$ 為預測值。
$$min{SUM(y_i-y_p)^2}$$
當誤差越大表示無法反映現實情況。我們以下使用 keras 和 sklearn 進行實驗。
程式碼 sklearn from tensorflow.keras.datasets import boston_housing (x_train, y_train), (x_test, y_test) = boston_housing.load_data() # 正規化 mean_feature_train = x_train.mean(axis=0) std_feature_train = x_train.std(axis=0) mean_feature_test = x_test.mean(axis=0) std_feature_test = x_test.std(axis=0) x_train = (x_train-mean_feature_train)/std_feature_train x_test = (x_test-mean_feature_test)/std_feature_test # 導入線性回歸模型 from sklearn.linear_model import LinearRegression regression = LinearRegression() regression = regression.</description></item><item><title>30 天學習歷程-day06</title><link>https://cch0124.github.io/blog/2020-08-24-data-clean/</link><pubDate>Sat, 22 Aug 2020 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/blog/2020-08-24-data-clean/</guid><description>Data cleaning 將使用 pandas 工具和 CICDDOS 數據集進行資料清理的練習。下面會學習到 pandas 的使用。
###　Drop Miss Value
import pandas as pd import numpy as np ddos = pd.read_csv(&amp;#34;D:\\DataSet\\CICDDOS2019\\01-12\\CSV\\DrDoS_LDAP.csv&amp;#34;) missing_values_count = ddos.isnull().sum() # 檢查缺失值 missing_values_count[20:25] ############################## #Bwd Packet Length Std 0 #Flow Bytes/s 12 # Flow Packets/s 0 # Flow IAT Mean 0 # Flow IAT Std 0 #dtype: int64 ##############################3 # Flow Bytes/s 有 12 個缺失值 ddos.shape # (2181542, 88) # 計算缺失值比例 total_miss_values = missing_values_count.</description></item><item><title>30 天學習歷程-day05</title><link>https://cch0124.github.io/blog/2020-08-21-data-pre-processing/</link><pubDate>Fri, 21 Aug 2020 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/blog/2020-08-21-data-pre-processing/</guid><description>在現今資料報炸時代，資料容易受到遺漏值或不一致性影響。這樣的影響會導致數據的品質降低，以至於對探勘結果會有不好的影響。因此資料前處裡是一個很重要的步驟，這將影響著數據研究結果。當今有許多資料前處裡的技術，像是資料清理、資料整合、降維和資料轉換等等。
為何需要前處裡 簡單來說就是要當前資料有價值的去被應用。
資料品質由許多要素組成：
正確性 刻意輸入不正確值或設備故障 人為不小心錯誤輸入 完整性 客戶填寫的資料非所有都填寫 某些資訊可能沒有其它欄位資訊 一致性 格式不一樣等 時效性 資料的某些訊息需要在某個時間才會被輸入 可信度 資料有多少程度被使用者信賴 可解讀性 資料能否被使用者所解讀 然而前三項是現今大型資料庫或倉儲時常遇到的問題。
資料前處裡任務 資料清理 清除資料中雜訊，修復不一致性。
遺漏值 忽略該值值組 人工補遺漏值 使用一個長數值代替遺漏值 使用屬性的平均值或中位數等來填補遺漏值 使用同一類別的樣本平均值或中位數等來填補遺漏值 使用迴歸或決策樹等決定該遺漏值 避免遺漏值的方法最好是定義好規則。
雜訊資料 分箱法 對資料做排序，並指定分箱個數，在使用平均值或邊界方法來平滑數據。
迴歸 找出能夠切分資料點的線獲曲面。
離群值分析 藉由分群。
資料整合 從不同來源的資料合並成連貫一致的資料庫。可能不同資料庫輸入同一性質的值，但欄位名稱不一致，或者輸入值不一樣。然而有些資訊也許從多個欄位來取得，這在做整合時，同時也減少資料的冗長。
而資料冗於性可透過卡方檢定或相關係數與共變異數等檢測。</description></item><item><title>30 天學習歷程-day04</title><link>https://cch0124.github.io/blog/2020-08-20-data-similarity-dissimilarity/</link><pubDate>Thu, 20 Aug 2020 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/blog/2020-08-20-data-similarity-dissimilarity/</guid><description>資料的相異性與相似性 在群集、離群值與最近鄰居等演算法應用中，需要比較兩個物件並評估兩物件之間的相似、相異性。
群集是指資料物件的集合，同一群集內的資料物件是彼此相似，不同群集間則是相異。離群值是辨識出那些物件與其它資料物件有著高度相異並將其認定為離群值。最近鄰居是對一個資料物件指定類別標籤，根據它與分類模型中其他物件的相似度決定。
資料矩陣和相異度矩陣 這邊的特徵向量將以二維或多維度屬性組成。
資料矩陣 使用 $n$ 筆物件乘上 $p$ 個屬性，來儲存 $n$ 筆資料物件。也稱屬性結構。
$\begin{bmatrix} x_{11} &amp;amp; &amp;hellip; &amp;amp; x_{1f} &amp;amp; &amp;hellip; &amp;amp; x_{1p} \
&amp;hellip; &amp;amp; &amp;hellip; &amp;amp; &amp;hellip; &amp;amp; &amp;hellip; &amp;amp; &amp;hellip; \
x_{i1} &amp;amp; &amp;hellip; &amp;amp; x_{if} &amp;amp; &amp;hellip; &amp;amp; x_{ip} \
&amp;hellip; &amp;amp; &amp;hellip; &amp;amp; &amp;hellip; &amp;amp; &amp;hellip; &amp;amp; &amp;hellip; \
x_{n1} &amp;amp; &amp;hellip; &amp;amp; x_{nf} &amp;amp; &amp;hellip; &amp;amp; x_{np} \
\end{bmatrix}$
相異度矩陣 儲存 $n$ 筆物件集合中，每一對物件的鄰近值，通常用一個 $n \times n$ 矩陣來表示。</description></item><item><title>30 天學習歷程-day03</title><link>https://cch0124.github.io/blog/2020-08-19-chart/</link><pubDate>Wed, 19 Aug 2020 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/blog/2020-08-19-chart/</guid><description>Data Visualization 透過直方圖或分位數圖等能有效的對一個特徵進行觀察，而兩個特徵可使用散佈圖，本篇文章將介紹一些用 python 工具進行視覺化的方法。
Quantile Plot 對於給定的特徵，會顯示所有資料，並透過此呈現觀察數據的不尋常處或行為。最後在標示分位數已進行分布的觀察。
Quantile-Quantile Plot 能夠檢視從一個變數分布到另一個變數分布時是否有偏移現象。
Histogram 直方圖是對某一個特徵摘要其資料分布，然而該長條的高度代表該特徵出現的頻率。
price_count = {40: 275, 43: 300, 47:250, 74: 360, 75: 515, 78: 540, 115:320, 117:270, 120:350} sum_60 = 0 sum_80 = 0 sum_100 = 0 sum_120 = 0 for key, values in price_count.items(): # key 為價格，values 產品銷售數量 if key &amp;gt; 40 and key &amp;lt; 60: sum_60 += price_count.get(key) if key &amp;gt;= 60 and key &amp;lt; 80: sum_80 += price_count.</description></item><item><title>30 天學習歷程-day02</title><link>https://cch0124.github.io/blog/2020-08-18-statistical-data/</link><pubDate>Tue, 18 Aug 2020 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/blog/2020-08-18-statistical-data/</guid><description>Statistical Data 透過統計方式我們可以觀察資料的趨勢或是分散程度等，以下將會介紹常用的統計方式。
數據集中趨勢 Mean 一組數據中的平衡點 平均數是一組樣本和除以樣本數量 另 $x_1, x_2,&amp;hellip;, x_N$ 為 $X$ 的 $N$ 個觀測值。這些值有可稱為數據集。這些數值均值(mean)為 $$\bar{x} = \frac{\sum_{i=1}^{N} x_i}{N} = \frac{x_1+x_2+&amp;hellip;+x_N}{N}$$
有時可以與 $w_i$ 權重相關。此反應他們所依附的對應值的意義、重要性或出現頻率。可以下計算： $$\bar{x} = \frac{\sum_{i=1}^{N} w_i x_i}{\sum_{i=1}^{N} w_i} = \frac{w_ix_1+w_2x_2+&amp;hellip;+w_nx_N}{w_1+w_2+&amp;hellip;+w_N}$$
這稱為加權是算術平均值（weight arithmetic mean）或加權平均值（weighted average）。
但上面計算的均值對極端值很敏感。要避免可以使用截尾均值(trimmed mean)，丟棄高低極端值得影響。在計算平均值之前，移除最底部 2% 資料等。
import numpy as np nums = [1,2,3,4,4,4,5,8,2,3] # Numpy np.mean(nums) # 3.6 # for sum = 0 for i in nums: sum += i print(sum/len(nums)) # 3.6 Median 樣本需要是排序 代表一個樣本或概率分佈中的一個數值，可將數值集合劃分為相等的上下兩部分 樣本為偶數個，則中位數不唯一，通常取最中間的兩個數值的平均值作為中位數 對於 偏斜（非對稱） 數據，數據中心更好度量是中位數(median)，將較高或較低值給平均。 有序數據值的中間值。</description></item><item><title>csv combine</title><link>https://cch0124.github.io/blog/2020-02-13-csv-combine/</link><pubDate>Thu, 13 Feb 2020 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/blog/2020-02-13-csv-combine/</guid><description>資料集有分為幾十種的種類，每一種類的資料大小都起碼 1G，由於深度學習時資料只有一個種類會不大理想，因此要將這些多個類型的檔案合併成一個。 我使用了 pandas，程式碼如下
import pandas as pd import os import glob csv_file_path = &amp;#39;DATA_ROOT_PATH&amp;#39; os.chdir(csv_file_path) list_of_file = [file for file in blob.glob(&amp;#39;DrDos_*.csv&amp;#39;)] combined_csv = pd.concat([pd.read_csv(f) for f in list_of_file[:-1]]) combined_csv.to_csv(&amp;#39;SAVE_PATH&amp;#39;, index=True) 個人配置是 32GB，但在處裡 2 個 4G 檔案做合併時就會發生記憶體不足的問體。問題點可能是
檔案真的太大 該檔案裡的資料型態佔據的記憶體空間。因為有時資料只有 0 或 1，卻用 int64 來儲存，那豈不是佔據空間 之後嘗試用，shell awk和管道來處理，該機器有 22G 記憶體空間，但在執行時，看不出記憶體變化程度，這優化不知道是怎麼回事&amp;hellip;。 之所以用 awk 是因為檔案中的 header 是一樣的，在合併時需要將該 header 省略，在配合著 &amp;gt;&amp;gt; 附加作用，即可完成。當然這種導向的輸出 cat、paste 等都可以做到，但因為沒省略 header 的方式，所以就沒用了。
awk &amp;#39;FNR &amp;gt; 1&amp;#39; file.csv file2.csv &amp;gt;&amp;gt; output.</description></item><item><title>ML-3-meanshift</title><link>https://cch0124.github.io/blog/2020-01-03-meanshift.md/</link><pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/blog/2020-01-03-meanshift.md/</guid><description>What is meanshift 資料集的密度為一個隨核密度分佈，能夠在此資料集中找到局部極值，即為一個 kernel density estimation（它不需要預先知道樣本數據的概率密度分佈函數，完全能夠對樣本點的計算），因此將資料分群。
Example import numpy as np import matplotlib.pyplot as plt import pandas as pd from sklearn.cluster import MeanShift, estimate_bandwidth from sklearn import datasets from sklearn import preprocessing #create datasets # iris = datasets.load_iris() # X = iris.data[:, :4] url = &amp;#34;https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv&amp;#34; data = pd.read_csv(url) le = preprocessing.LabelEncoder() data[&amp;#39;species&amp;#39;] = le.fit_transform(data.iloc[:,-1]) X = data.iloc[:, 0:4].to_numpy() y = data.iloc[:,-1].to_numpy() plt.scatter(X[:, 0], X[:, 1], c=&amp;#34;yellow&amp;#34;, marker=&amp;#39;o&amp;#39;, label=&amp;#39;see&amp;#39;) plt.</description></item><item><title>ML-2-DBSCAN</title><link>https://cch0124.github.io/blog/2020-01-02-dbscan/</link><pubDate>Thu, 02 Jan 2020 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/blog/2020-01-02-dbscan/</guid><description>What is DBSCAN DBSCAN（Density-based spatial clustering of applications with noise）是一種基於密度的演算法。給定一個數量點的閾值，表示該群集要超過一定的密度程度。密度程度則會透過距離方式來做計算。
Concept Parameters Eps 鄰域的最大半徑 MinPts 點的 Eps-neighborhood 中的最少點數量 Density Definition $\varepsilon$-Neighborhood 距離對象半徑 $\varepsilon$ 以內的對象。 距離 $\varepsilon$ 內的所有點的集合。 $N_{\varepsilon}(p):{q|d(p,q) \leq \varepsilon}$
High density 一個對象的 $\varepsilon$-Neighborhood 至少包含 MinPts 個對象。 ${\varepsilon}$-Neighborhood of $p$ ${\varepsilon}$-Neighborhood of $q$ Density of p is high (MinPts = 4) Density of q is low (MinPts = 4)</description></item><item><title>ML day1 k-means</title><link>https://cch0124.github.io/blog/2019-12-31-k-means/</link><pubDate>Tue, 31 Dec 2019 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/blog/2019-12-31-k-means/</guid><description>What is K-means 物以類聚的概念，K-means 的 K 就是幾群的意思。利用距離和群心的計算去完成聚類的任務。
K-means Algorithm 先設定 K 要分為幾群 輸入特徵 為 K 個群心計算 Euclidean distance 把每個資料分群至距離最短的該群心 重新計算各群的群心 不斷重複 3-5，直到收斂 Python sklearn example import pandas as pd import numpy as np import sklearn.metrics as sm import matplotlib.pyplot as plt from sklearn.cluster import KMeans from sklearn import datasets from sklearn import preprocessing url = &amp;#34;https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv&amp;#34; data = pd.read_csv(url) le = preprocessing.LabelEncoder() data[&amp;#39;species&amp;#39;] = le.fit_transform(data.iloc[:,-1]) X = data.iloc[:, 0:4].</description></item><item><title>supervised vs unsupervised Learning</title><link>https://cch0124.github.io/blog/2019-12-31-supervised-unsupervised/</link><pubDate>Tue, 31 Dec 2019 00:00:00 +0000</pubDate><guid>https://cch0124.github.io/blog/2019-12-31-supervised-unsupervised/</guid><description>What is supervised ? 有存在正確答案的的數據，簡而言之就是有&amp;quot;Label&amp;quot;。監督式學習從有 Label 的數據學習建立出可預測的模型。例如：天氣狀況、物件辨識等。
流程：
Training Data -&amp;gt; Features Selection -&amp;gt; Algorithm -&amp;gt; Model Type Regression 預測一個數值 Classification 將輸出分組到一個類中 from : [http://www.slideshare.net/datascienceth/machine-learning-in-image-processing]
What is unsupervised ? 從無 Label 中的數據，自行找出資料的結構建立模型。相比監督式學習，無監督更加不可預測。
Type Clustering 從數據中找出結構或模式，並將其自然地作出聚類 Association 在大型的數據資料中找出數據對象之間的關聯 例子 購物的人瀏覽和購買物品的組合 from: [https://medium.com/data-science-by-heart/types-of-learning-93b721d5af91]</description></item></channel></rss>